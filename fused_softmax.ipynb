{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fcaff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af93e947",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CudaDriver' object has no attribute 'get_active_torch_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_active_torch_device\u001b[49m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_hip\u001b[39m():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m triton\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_current_target()\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/triton/runtime/driver.py:24\u001b[0m, in \u001b[0;36mLazyProxy.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_obj()\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CudaDriver' object has no attribute 'get_active_torch_device'"
     ]
    }
   ],
   "source": [
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3db43e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch softmax function\n",
    "\n",
    "@torch.jit.script\n",
    "def naive_softmax(x):\n",
    "    \"\"\"\n",
    "    compute row-wise softmax of X using native pytorch\n",
    "    \"\"\"\n",
    "    # prevent overflow, max = 0 in each row\n",
    "    x_max = x.max(dim=1)[0]\n",
    "    z = x - x_max[:, None]\\\n",
    "    \n",
    "    # normalize for softmax\n",
    "    numerator = torch.exp(z)  # exponent of element\n",
    "    denominator = numerator.sum(dim=1)  # sum each row\n",
    "    \n",
    "    # 2D tensor of softmax probabilities\n",
    "    ret = numerator / denominator[:, None]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0369148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    output_ptr,\n",
    "    input_ptr, \n",
    "    input_row_stride, \n",
    "    output_row_stride,\n",
    "    n_rows,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "    num_stages: tl.constexpr\n",
    "):\n",
    "    row_start = tl.program_id(0)  # get row id\n",
    "    row_step = tl.num_programs(0)  # number of programs to process\n",
    "    \n",
    "    # run program for each row\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages): \n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        \n",
    "        # mask so extra threads do nothing\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)  \n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        mask = col_offsets < n_cols\n",
    "\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        row_minus_max = row - tl.max(row, axis=0)  # remove max\n",
    "        \n",
    "        # normalize\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        \n",
    "        # configure and store output\n",
    "        output_row_start_ptr = output_ptr + row_idx + output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fb89e9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DEVICE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from Triton documentation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m properties \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_device_properties(\u001b[43mDEVICE\u001b[49m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m      4\u001b[0m NUM_SM \u001b[38;5;241m=\u001b[39m properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessor_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m NUM_REGS \u001b[38;5;241m=\u001b[39m properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_num_regs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEVICE' is not defined"
     ]
    }
   ],
   "source": [
    "# from Triton documentation\n",
    "\n",
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # Another trick we can use is to ask the compiler to use more threads per row by\n",
    "    # increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # You will see in the next tutorial how to auto-tune this value in a more natural\n",
    "    # way so you don't have to come up with manual heuristics yourself.\n",
    "    num_warps = 8\n",
    "\n",
    "    # Number of software pipelining stages.\n",
    "    num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # pre-compile kernel to get register usage and compute thread occupancy.\n",
    "    kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    size_smem = kernel.metadata.shared\n",
    "    if is_hip():\n",
    "        # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "        # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "        # ISA SECTION (3.6.4 for CDNA3)\n",
    "        # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "        # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "        # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "        # not required to be equal numbers of both types.\n",
    "        if is_cdna():\n",
    "            NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "        # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "        # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "        # execute on a CU (multi-processor)  in parallel.\n",
    "        MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "        max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "        occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "    else:\n",
    "        occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "    num_programs = NUM_SM * occupancy\n",
    "\n",
    "    num_programs = min(num_programs, n_rows)\n",
    "\n",
    "    # Create a number of persistent programs.\n",
    "    kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67db3b63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DEVICE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1823\u001b[39m, \u001b[38;5;241m781\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[43mDEVICE\u001b[49m)\n\u001b[1;32m      3\u001b[0m y_triton \u001b[38;5;241m=\u001b[39m softmax(x)\n\u001b[1;32m      4\u001b[0m y_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEVICE' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(1823, 781, device=DEVICE)\n",
    "y_triton = softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c744d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
